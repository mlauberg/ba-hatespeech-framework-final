# Iterative Prompt Optimization for Hate Speech Detection

Begleitcode zur Bachelorarbeit **"Iterative Promptoptimierung zur Klassifikation von Hassrede"** (2025).

---

## ğŸ“– Ãœberblick

Dieses Framework implementiert einen **automatisierten Feedback-Zyklus** zur Optimierung von System-Prompts fÃ¼r Large Language Models (LLMs) im Kontext der deutschsprachigen Hate Speech Detection.

### Methodischer Ansatz

Das Framework arbeitet in **3 Phasen**:

1. **Phase 1: Initialisierung**
   - Baseline-Prompt (Zero-Shot oder manuell)
   - Stratifizierte Datenaufteilung (Train/Val/Test)
   - Baseline-Evaluation auf Validierungsdaten (Iteration 0)

2. **Phase 2: Iterative Optimierung** (Hauptschleife)
   - Inferenz auf Validierungsdaten (Google Gemma 3 27B)
   - Fehleranalyse (False Positives & False Negatives)
   - Prompt-Verbesserung durch Meta-LLM
   - Tracking des besten Prompts (S-Score als Zielfunktion)
   - **Testdaten werden NICHT berÃ¼hrt** (Held-Out-Set)

3. **Phase 3: Finale Evaluation**
   - Einmalige Evaluation auf unabhÃ¤ngigem Testdatensatz
   - Vergleich: Baseline vs. optimierter Prompt
   - Statistische SignifikanzprÃ¼fung (McNemar-Test)

---

## ğŸš€ Installation

### Voraussetzungen

- **Python 3.12+**
- **Poetry** (Dependency Management)
- **Ollama** (fÃ¼r lokale LLM-Inferenz): https://ollama.ai/

### Schritt 1: Repository klonen

```bash
git clone https://github.com/mlauberg/ba-hatespeech-framework-final.git
cd ba-hatespeech-framework-final
```

### Schritt 2: Dependencies installieren

```bash
# Poetry installieren (falls nicht vorhanden)
curl -sSL https://install.python-poetry.org | python3 -

# AbhÃ¤ngigkeiten installieren
poetry install

# Virtual Environment aktivieren
poetry shell
```

### Schritt 3: LLM-Modell vorbereiten

```bash
# Ollama muss laufen (separates Terminal)
ollama serve

# Modell herunterladen (einmalig, ~15 GB)
ollama pull gemma2:27b
```

---

## ğŸ“Š Verwendung

### 1. Datensatz vorbereiten

Legen Sie Ihren Datensatz im Ordner `data/` ab:

```
data/
â””â”€â”€ raw/
    â””â”€â”€ gutefrage.csv    # Oder HOCON34k
```

**Erforderliche Spalten:**
- `text`: Textinhalt
- `label`: BinÃ¤res Label (0 = NOT-HS, 1 = HS)

### 2. Konfiguration anpassen

In `main.py` (Zeilen 74-86):

```python
DATA_FILE = "data/raw/gutefrage.csv"    # Pfad zum Datensatz
DATASET_TYPE = "gutefrage"              # Oder "hocon"
SAMPLE_SIZE = 100                       # StichprobengrÃ¶ÃŸe
MAX_ITERATIONS = 100                    # Optimierungszyklen
MAX_ERRORS_TO_ANALYZE = 10              # Fehler pro Iteration

INITIAL_PROMPT = """..."""              # Baseline-Prompt
```

### 3. Experiment starten

```bash
poetry run python main.py
```

**Ausgabe wÃ¤hrend der AusfÃ¼hrung:**
- Progress Bar fÃ¼r jede Iteration
- Metriken nach jedem Zyklus (F1, F2, MCC, S-Score)
- ETA (geschÃ¤tzte Restlaufzeit)
- Automatische Speicherung der Ergebnisse

### 4. Ergebnisse auswerten

Nach Abschluss finden Sie im Ordner `results/`:

- `experiment_YYYYMMDD_HHMMSS.csv`: Metriken pro Iteration
- `experiment_YYYYMMDD_HHMMSS.json`: VollstÃ¤ndiger Prompt-Verlauf
- `best_prompt.txt`: Bester gefundener Prompt

---

## ğŸ“ˆ Metriken

Das Framework verwendet eine **kombinierte Zielfunktion** (S-Score):

### S-Score = F2-Score + MCC

- **F2-Score**: Priorisiert Recall (Î²=2)
  â†’ Vermeidung von False Negatives (Ã¼bersehene Hassrede)

- **MCC**: Matthews Correlation Coefficient
  â†’ Robustheit bei Klassenungleichgewicht

**ZusÃ¤tzlich berechnet:**
- F1-Score
- Accuracy
- Precision
- Recall

---

## ğŸ—ï¸ Projektstruktur

```
ba-hatespeech-framework-final/
â”œâ”€â”€ README.md                   # Diese Datei
â”œâ”€â”€ LICENSE                     # MIT License
â”œâ”€â”€ pyproject.toml              # Dependencies (Poetry)
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ main.py                     # Hauptschleife (3 Phasen)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py              # Metrik-Berechnung (F2, MCC, S-Score)
â”‚   â”œâ”€â”€ optimizer.py            # Error-driven Meta-Prompting
â”‚   â”œâ”€â”€ inference.py            # LLM-Inferenz (Ollama-Integration)
â”‚   â””â”€â”€ data_loader.py          # Datenverarbeitung & Stratified Split
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ .gitkeep
    â””â”€â”€ raw/                    # DatensÃ¤tze hier ablegen
```

---

## ğŸ”¬ Wissenschaftlicher Hintergrund

### Kernidee: Error-Driven Meta-Prompting

Anstatt Prompts manuell zu optimieren, analysiert ein **Meta-LLM** (derselbe Gemma 3 27B) die Fehler einer Iteration und generiert **synthetische Few-Shot-Beispiele**, um die Entscheidungsgrenze zu schÃ¤rfen.

**Stateless-Design:**
- Der Optimizer hat **keinen Zugriff** auf die Historie
- Jede Iteration basiert nur auf: `aktueller Prompt + aktuelle Fehler`
- Verhindert "Prompt Bloat" durch unkontrolliertes AnhÃ¤ngen

### DatensÃ¤tze

- **gutefrage.net**: Q&A-Plattform (informelle Sprache, Slang)
- **HOCON34k**: Zeitungskommentare (formeller Kontext)

â†’ Cross-Domain-Validierung zur PrÃ¼fung der GeneralisierungsfÃ¤higkeit

---

## ğŸ“ Zitation

Falls Sie diese Arbeit in Ihrer Forschung verwenden:

```bibtex
@thesis{lauberger2025,
  author = {Maximilian Lauberger},
  title = {Iterative Promptoptimierung zur Klassifikation von Hassrede},
  school = {Hochschule MÃ¼nchen, FakultÃ¤t fÃ¼r Informatik und Mathematik},
  year = {2025},
  type = {Bachelor's Thesis},
  url = {https://github.com/mlauberg/ba-hatespeech-framework-final}
}
```

---

## ğŸ› ï¸ Technische Details

### Hardware-Anforderungen

- **GPU**: Mind. 20 GB VRAM (fÃ¼r Gemma 3 27B in Q4-Quantisierung)
- **RAM**: 32 GB empfohlen
- **Speicher**: ~50 GB fÃ¼r Modell + DatensÃ¤tze

### Software-Stack

- **Python 3.12**
- **Ollama** (LLM-Inferenz-Server)
- **scikit-learn** (Metriken)
- **pandas** (Datenverarbeitung)
- **tqdm** (Progress Bar)

---

## ğŸ¤ Beitragen

Dieses Repository dient primÃ¤r der **Reproduzierbarkeit** der Bachelorarbeit.
FÃ¼r Fragen oder Anregungen:

- **Autor**: Maximilian Lauberger
- **E-Mail**: lauberge@hm.edu
- **Betreuer**: Prof. Dr. Peter Mandl (Hochschule MÃ¼nchen)

---

## ğŸ“„ Lizenz

MIT License - Siehe [LICENSE](LICENSE) fÃ¼r Details.

---

## ğŸ™ Danksagung

Dank an:
- Prof. Dr. Peter Mandl fÃ¼r die wissenschaftliche Betreuung
- Die Entwickler von Ollama und Google Gemma fÃ¼r die Open-Source-Modelle
- Die Ersteller der DatensÃ¤tze gutefrage.net und HOCON34k

---

## ğŸ”— WeiterfÃ¼hrende Links

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Google Gemma Models](https://ai.google.dev/gemma)
- [Poetry Documentation](https://python-poetry.org/)
