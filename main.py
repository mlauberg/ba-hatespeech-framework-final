"""
Iterative Prompt Optimization Framework for Hate Speech Detection.

This module implements the complete three-phase optimization framework as described
in the thesis methodology:

Phase 1: Initialization (Baseline Evaluation)
    - Initial prompt creation (zero-shot or manual few-shot)
    - Stratified train/val/test split (CRITICAL: prevents data leakage)
    - Baseline evaluation on VALIDATION set (logged as iteration 0)

Phase 2: Iterative Optimization Cycle
    - Gemma inference with batch processing on VALIDATION set
    - Metric calculation (F1, F2, MCC, Accuracy)
    - Convergence check (max iterations or metric plateau)
    - Error analysis and example extraction from VALIDATION set
    - Optimizer LLM generates improved prompt with strategic few-shot selection
    - Track best-performing prompt across iterations
    - Loop continues until convergence
    - TEST SET NEVER TOUCHED during this phase

Phase 3: Final Validation
    - Single evaluation on held-out TEST set with best prompt
    - Final results and thesis reporting
    - Comparison: Baseline vs. Optimized (both on test set)

Key Features:
    - STRICT DATA SPLITTING: Prevents optimization from seeing test data
    - Standard metrics (F1, F2, MCC, Accuracy) suitable for peer review
    - Crash-resistant incremental data persistence
    - Multi-dataset support (gutefrage.net and HOCON34k)
    - Baseline comparison tracking
    - Automated visualization generation

CRITICAL SCIENTIFIC REQUIREMENT:
    The test set is held out completely during optimization (Phase 2) and used
    exactly once at the end (Phase 3). This ensures the reported performance
    represents genuine generalization, not overfitting to evaluation data.

Author: Bachelor Thesis Project
Date: 2025
"""

import csv
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy import stats
from tqdm import tqdm

from src.data_loader import load_and_prepare_data, split_data
from src.inference import classify_text
from src.optimizer import generate_improved_prompt
from src.metrics import calculate_metrics

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(levelname)s] %(asctime)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

# Experimental Configuration
DATA_FILE: str = "data/raw/gutefrage.csv"
DATASET_TYPE: str = "gutefrage"  # Options: 'gutefrage' or 'hocon'
SAMPLE_SIZE: int = 10
MAX_ITERATIONS: int = 2
MAX_ERRORS_TO_ANALYZE: int = 10

INITIAL_PROMPT: str = """You are a content moderator evaluating German text for hate speech.
Analyze the following text and decide whether it is hate speech (HS) or not hate speech (NOT-HS).

DEFINITION:
Hate Speech = Attacks on groups based on race, religion, origin, or gender.
Insults targeting individuals without group reference = NOT hate speech."""


def format_time_remaining(seconds: float) -> str:
    """
    Format seconds into human-readable time string.

    Args:
        seconds: Time in seconds to format

    Returns:
        Formatted string like "1h 30m" or "45m" or "30s"
    """
    if seconds < 60:
        return f"{int(seconds)}s"
    elif seconds < 3600:
        minutes = int(seconds / 60)
        return f"{minutes}m"
    else:
        hours = int(seconds / 3600)
        minutes = int((seconds % 3600) / 60)
        return f"{hours}h {minutes}m"


def bootstrap_confidence_interval(
    y_true: List[int],
    y_pred: List[int],
    metric_name: str = 'F2',
    n_bootstrap: int = 1000,
    confidence_level: float = 0.95
) -> Tuple[float, float, float]:
    """
    Calculate bootstrap confidence interval for a classification metric.

    Uses bootstrap resampling to estimate the confidence interval for a metric,
    providing statistical rigor for thesis reporting. This addresses the question:
    "How uncertain is this metric estimate?"

    Args:
        y_true: Ground truth labels
        y_pred: Predicted labels
        metric_name: Metric to calculate CI for ('F1', 'F2', 'MCC', 'ACC')
        n_bootstrap: Number of bootstrap samples (default: 1000)
        confidence_level: Confidence level (default: 0.95 for 95% CI)

    Returns:
        Tuple of (point_estimate, lower_bound, upper_bound)

    Example:
        >>> f2, lower, upper = bootstrap_confidence_interval(y_true, y_pred, 'F2')
        >>> print(f"F2: {f2:.4f} [{lower:.4f}, {upper:.4f}]")
    """
    n = len(y_true)
    bootstrap_metrics = []

    # Calculate point estimate
    point_metrics = calculate_metrics(y_true, y_pred)
    point_estimate = point_metrics[metric_name]

    # Bootstrap resampling
    rng = np.random.RandomState(42)  # Reproducibility
    for _ in range(n_bootstrap):
        # Resample with replacement
        indices = rng.choice(n, size=n, replace=True)
        y_true_boot = [y_true[i] for i in indices]
        y_pred_boot = [y_pred[i] for i in indices]

        # Calculate metric on bootstrap sample
        boot_metrics = calculate_metrics(y_true_boot, y_pred_boot)
        bootstrap_metrics.append(boot_metrics[metric_name])

    # Calculate confidence interval
    alpha = 1 - confidence_level
    lower_percentile = (alpha / 2) * 100
    upper_percentile = (1 - alpha / 2) * 100

    lower_bound = np.percentile(bootstrap_metrics, lower_percentile)
    upper_bound = np.percentile(bootstrap_metrics, upper_percentile)

    return point_estimate, lower_bound, upper_bound


def mcnemar_test(
    y_true: List[int],
    y_pred_baseline: List[int],
    y_pred_optimized: List[int]
) -> Tuple[float, str]:
    """
    Perform McNemar's test for statistical significance of improvement.

    Tests the null hypothesis: "The two models have equal error rates."
    A low p-value (< 0.05) indicates the improvement is statistically significant.

    McNemar's test is appropriate for paired predictions (same test set evaluated
    by two different models). It creates a contingency table:

                        Optimized Correct | Optimized Wrong
    Baseline Correct    both_correct      | baseline_only
    Baseline Wrong      optimized_only    | both_wrong

    The test focuses on discordant pairs (baseline_only vs optimized_only).

    Args:
        y_true: Ground truth labels
        y_pred_baseline: Baseline model predictions
        y_pred_optimized: Optimized model predictions

    Returns:
        Tuple of (p_value, interpretation_string)

    Example:
        >>> p_value, interp = mcnemar_test(y_true, y_baseline, y_optimized)
        >>> print(f"McNemar's test: p={p_value:.4f} â†’ {interp}")
    """
    # Create correctness arrays
    baseline_correct = [yt == yp for yt, yp in zip(y_true, y_pred_baseline)]
    optimized_correct = [yt == yp for yt, yp in zip(y_true, y_pred_optimized)]

    # Build contingency table
    both_correct = sum(b and o for b, o in zip(baseline_correct, optimized_correct))
    both_wrong = sum(not b and not o for b, o in zip(baseline_correct, optimized_correct))
    baseline_only = sum(b and not o for b, o in zip(baseline_correct, optimized_correct))
    optimized_only = sum(not b and o for b, o in zip(baseline_correct, optimized_correct))

    # McNemar's test focuses on discordant pairs
    # If one or both counts are small, use exact binomial test (more conservative)
    n_discordant = baseline_only + optimized_only

    if n_discordant == 0:
        return 1.0, "No disagreements between models (cannot test significance)"

    # Exact McNemar test (binomial test on discordant pairs)
    # Null hypothesis: baseline_only and optimized_only are equally likely
    p_value = stats.binomtest(k=optimized_only, n=n_discordant, p=0.5, alternative='greater').pvalue

    # Interpretation
    if p_value < 0.001:
        interpretation = "Highly significant improvement (p < 0.001)"
    elif p_value < 0.01:
        interpretation = "Very significant improvement (p < 0.01)"
    elif p_value < 0.05:
        interpretation = "Significant improvement (p < 0.05)"
    elif p_value < 0.10:
        interpretation = "Marginally significant improvement (p < 0.10)"
    else:
        interpretation = "No significant improvement (p â‰¥ 0.10)"

    logging.info(f"McNemar's test contingency table:")
    logging.info(f"  Both correct: {both_correct}")
    logging.info(f"  Baseline only correct: {baseline_only}")
    logging.info(f"  Optimized only correct: {optimized_only}")
    logging.info(f"  Both wrong: {both_wrong}")

    return p_value, interpretation


def run_baseline_evaluation(df: pd.DataFrame, prompt: str) -> Dict[str, float]:
    """
    Execute Phase 1: Baseline evaluation with initial prompt.

    Runs a single evaluation pass with the initial prompt to establish a
    performance baseline for comparison with optimized iterations. This
    corresponds to Phase 1 (Initialisierung) in the methodological flowchart.

    Args:
        df: Dataset DataFrame with columns: text, label, source_info
        prompt: Initial system prompt to evaluate

    Returns:
        Dictionary containing baseline metrics:
            - F1, F2, MCC, ACC (from calculate_metrics)
            - y_true, y_pred (for confusion matrix calculation)
    """
    logging.info("=" * 70)
    logging.info("PHASE 1: BASELINE EVALUATION (on validation set)")
    logging.info("=" * 70)

    y_true: List[int] = []
    y_pred: List[int] = []

    for _, row in tqdm(df.iterrows(), total=len(df), desc="Baseline classification"):
        text: str = row['text']
        true_label: int = row['label']

        result: Dict[str, Any] = classify_text(text, prompt)
        pred_label: int = result['label']

        y_true.append(true_label)
        y_pred.append(pred_label)

    baseline_metrics = calculate_metrics(y_true, y_pred)
    baseline_metrics['y_true'] = y_true
    baseline_metrics['y_pred'] = y_pred

    logging.info("Baseline results (validation set):")
    logging.info(f"  F1-Score:  {baseline_metrics['F1']:.4f}")
    logging.info(f"  F2-Score:  {baseline_metrics['F2']:.4f}")
    logging.info(f"  MCC:       {baseline_metrics['MCC']:.4f}")
    logging.info(f"  S-Score:   {baseline_metrics['S_SCORE']:.4f} (F2 + MCC)")
    logging.info(f"  Accuracy:  {baseline_metrics['ACC']:.4f}")

    return baseline_metrics


def plot_results(csv_path: Path) -> None:
    """
    Generate and save visualization of metric progression over iterations.

    Creates a comprehensive line chart showing the evolution of F1, F2, MCC,
    and Accuracy across all iterations (including baseline as iteration 0).
    Each metric uses a distinct color and marker style for clarity.

    Args:
        csv_path: Path to the CSV file containing experimental results

    Raises:
        FileNotFoundError: If CSV file does not exist
        ValueError: If CSV file is empty or malformed
    """
    logging.info("Generating visualization...")

    try:
        df = pd.read_csv(csv_path)
    except FileNotFoundError:
        logging.error(f"CSV file not found: {csv_path}")
        raise
    except pd.errors.EmptyDataError:
        logging.error("CSV file is empty")
        raise ValueError("CSV file is empty")

    if df.empty:
        logging.error("No data available for plotting")
        raise ValueError("No data in CSV file")

    required_columns = ['iteration', 'f1', 'f2', 'mcc', 's_score', 'accuracy']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        logging.error(f"Missing columns in CSV: {missing_columns}")
        raise ValueError(f"Missing required columns: {missing_columns}")

    plt.figure(figsize=(14, 8))

    plt.plot(df['iteration'], df['f1'], marker='o', linewidth=2,
             label='F1-Score', color='#1f77b4', markersize=8)
    plt.plot(df['iteration'], df['f2'], marker='s', linewidth=2,
             label='F2-Score (Î²=2, Recall-Optimized)', color='#ff7f0e', markersize=8)
    plt.plot(df['iteration'], df['mcc'], marker='^', linewidth=2,
             label='MCC (Matthews Correlation)', color='#2ca02c', markersize=8)
    plt.plot(df['iteration'], df['s_score'], marker='*', linewidth=2.5,
             label='S-Score (F2 + MCC, Optimization Target)', color='#9467bd', markersize=10)
    plt.plot(df['iteration'], df['accuracy'], marker='D', linewidth=2,
             label='Accuracy', color='#d62728', markersize=8)

    # Highlight baseline (iteration 0)
    plt.axvline(x=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)
    plt.text(0.1, 0.98, 'Baseline', transform=plt.gca().get_xaxis_transform(),
             fontsize=9, color='gray', va='top')

    plt.xlabel('Iteration', fontsize=12, fontweight='bold')
    plt.ylabel('Score', fontsize=12, fontweight='bold')
    plt.title('Metric Progression Over Iterations (Phase 2: Optimization on Validation Set)',
              fontsize=14, fontweight='bold')
    plt.legend(loc='best', fontsize=10)
    plt.grid(True, alpha=0.3, linestyle='--')
    plt.ylim(-0.1, 2.05)  # Extended to accommodate S-Score (F2 + MCC, max â‰ˆ 2)

    plt.xticks(df['iteration'])

    plt.tight_layout()

    plot_path = csv_path.parent / f"{csv_path.stem}_plot.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()

    logging.info(f"Visualization saved to: {plot_path}")


class ResultsLogger:
    """
    Comprehensive data persistence system for iterative prompt optimization experiments.

    This class implements a multi-format logging strategy designed for scientific
    reproducibility and thesis evaluation:

    Output Formats:
        - CSV: Quantitative metrics (F1, F2, MCC, accuracy, precision,
               recall, confusion matrix)
        - JSON: Complete prompt provenance, detailed error analysis, and baseline
        - TXT: Human-readable summary statistics with baseline comparison

    The logger implements crash-resistant incremental saving, ensuring no data loss
    in case of interruption during long experimental runs.

    Attributes:
        results_dir (Path): Directory for output files
        experiment_id (str): Unique identifier for this experimental run
        csv_path (Path): Path to CSV metrics file
        json_path (Path): Path to JSON provenance file
        summary_path (Path): Path to summary report file
        full_history (Dict): Complete experimental history in memory
    """

    def __init__(self, experiment_name: Optional[str] = None) -> None:
        """
        Initialize the results logger and create output files.

        Args:
            experiment_name: Optional custom name for the experiment.
                           If None, generates timestamp-based identifier.
        """
        self.results_dir: Path = Path("results")
        self.results_dir.mkdir(exist_ok=True)

        timestamp: str = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.experiment_id: str = experiment_name or f"experiment_{timestamp}"

        self.csv_path: Path = self.results_dir / f"{self.experiment_id}.csv"
        self.json_path: Path = self.results_dir / f"{self.experiment_id}_full.json"
        self.summary_path: Path = self.results_dir / f"{self.experiment_id}_summary.txt"

        self.full_history: Dict[str, Any] = {
            "experiment_id": self.experiment_id,
            "start_time": timestamp,
            "config": {
                "data_file": DATA_FILE,
                "dataset_type": DATASET_TYPE,
                "sample_size": SAMPLE_SIZE,
                "max_iterations": MAX_ITERATIONS,
                "max_errors_analyzed": MAX_ERRORS_TO_ANALYZE
            },
            "baseline": None,
            "iterations": []
        }

        with open(self.csv_path, 'w', newline='', encoding='utf-8') as csv_file:
            writer = csv.writer(csv_file)
            writer.writerow([
                'iteration', 'f1', 'f2', 'mcc', 's_score', 'accuracy', 'precision', 'recall',
                'tp', 'tn', 'fp', 'fn', 'errors_count', 'prompt_length', 'prompt_preview'
            ])

        logging.info("Results logger initialized")
        logging.info(f"CSV output:     {self.csv_path}")
        logging.info(f"JSON output:    {self.json_path}")
        logging.info(f"Summary output: {self.summary_path}")

    def save_baseline(self, baseline_metrics: Dict[str, float]) -> None:
        """
        Save baseline metrics for comparison tracking.

        Stores baseline evaluation results and persists them to JSON for
        future reference and comparison with optimized iterations.

        Args:
            baseline_metrics: Dictionary with F1, F2, MCC, ACC, S_SCORE
        """
        self.full_history["baseline"] = {
            "F1": baseline_metrics['F1'],
            "F2": baseline_metrics['F2'],
            "MCC": baseline_metrics['MCC'],
            "ACC": baseline_metrics['ACC'],
            "S_SCORE": baseline_metrics['S_SCORE']
        }

        with open(self.json_path, 'w', encoding='utf-8') as json_file:
            json.dump(self.full_history, json_file, indent=2, ensure_ascii=False)

        logging.info("Baseline metrics saved to JSON")

    def log_iteration(
        self,
        iteration: int,
        metrics: Dict[str, Any],
        prompt: str,
        errors: List[Dict[str, Any]]
    ) -> None:
        """
        Persist iteration results to disk (CSV and JSON).

        This method implements crash-resistant saving by writing to both CSV
        (append mode) and JSON (full rewrite) after each iteration. If the
        process crashes, all completed iterations are preserved.

        Args:
            iteration: Current iteration number (0 = baseline, 1+ = optimized)
            metrics: Dictionary containing 'F1', 'F2', 'MCC', 'ACC',
                    'y_true', 'y_pred'
            prompt: Current system prompt text
            errors: List of error dictionaries with keys 'type', 'text',
                   'true_label', 'pred_label'
        """
        y_true: List[int] = metrics['y_true']
        y_pred: List[int] = metrics['y_pred']

        true_positive: int = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)
        true_negative: int = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)
        false_positive: int = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)
        false_negative: int = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)

        precision: float = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0
        recall: float = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0.0

        with open(self.csv_path, 'a', newline='', encoding='utf-8') as csv_file:
            writer = csv.writer(csv_file)
            writer.writerow([
                iteration,
                f"{metrics['F1']:.4f}",
                f"{metrics['F2']:.4f}",
                f"{metrics['MCC']:.4f}",
                f"{metrics['S_SCORE']:.4f}",
                f"{metrics['ACC']:.4f}",
                f"{precision:.4f}",
                f"{recall:.4f}",
                true_positive, true_negative, false_positive, false_negative,
                len(errors),
                len(prompt),
                prompt[:100].replace('\n', ' ')
            ])

        iteration_record: Dict[str, Any] = {
            "iteration": iteration,
            "metrics": {
                "F1": metrics['F1'],
                "F2": metrics['F2'],
                "MCC": metrics['MCC'],
                "S_SCORE": metrics['S_SCORE'],
                "ACC": metrics['ACC'],
                "precision": precision,
                "recall": recall,
                "confusion_matrix": {
                    "tp": true_positive,
                    "tn": true_negative,
                    "fp": false_positive,
                    "fn": false_negative
                }
            },
            "prompt": prompt,
            "errors": errors[:20]
        }
        self.full_history["iterations"].append(iteration_record)

        with open(self.json_path, 'w', encoding='utf-8') as json_file:
            json.dump(self.full_history, json_file, indent=2, ensure_ascii=False)

    def generate_summary(self) -> None:
        """
        Generate comprehensive summary statistics for thesis evaluation.

        Produces a human-readable report containing:
        - Experimental configuration parameters
        - Baseline performance metrics
        - Performance metrics (initial, final, best, worst, mean) for all metrics
        - Baseline comparison with improvement percentages
        - Prompt evolution statistics
        - Provenance information for data files

        The summary is saved to a text file and logged to console.
        """
        iterations: List[Dict[str, Any]] = self.full_history["iterations"]

        if not iterations:
            logging.warning("No data available for summary generation")
            return

        f1_scores: List[float] = [it["metrics"]["F1"] for it in iterations]
        f2_scores: List[float] = [it["metrics"]["F2"] for it in iterations]
        mcc_scores: List[float] = [it["metrics"]["MCC"] for it in iterations]
        s_scores: List[float] = [it["metrics"]["S_SCORE"] for it in iterations]
        accuracy_scores: List[float] = [it["metrics"]["ACC"] for it in iterations]

        best_f1_index, best_f1_value = max(enumerate(f1_scores), key=lambda x: x[1])
        worst_f1_index, worst_f1_value = min(enumerate(f1_scores), key=lambda x: x[1])
        best_f2_index, best_f2_value = max(enumerate(f2_scores), key=lambda x: x[1])
        best_s_index, best_s_value = max(enumerate(s_scores), key=lambda x: x[1])

        # Baseline comparison
        baseline = self.full_history.get("baseline")
        baseline_comparison = ""
        if baseline:
            f1_improvement = f1_scores[-1] - baseline['F1']
            f1_improvement_pct = (f1_improvement / baseline['F1'] * 100) if baseline['F1'] != 0 else 0

            f2_improvement = f2_scores[-1] - baseline['F2']
            f2_improvement_pct = (f2_improvement / baseline['F2'] * 100) if baseline['F2'] != 0 else 0

            s_improvement = s_scores[-1] - baseline['S_SCORE']
            s_improvement_pct = (s_improvement / baseline['S_SCORE'] * 100) if baseline['S_SCORE'] != 0 else 0

            baseline_comparison = f"""
BASELINE COMPARISON (Phase 1 vs Final):
  Metric          Baseline    Final       Improvement
  F1-Score:       {baseline['F1']:.4f}      {f1_scores[-1]:.4f}      {f1_improvement:+.4f} ({f1_improvement_pct:+.1f}%)
  F2-Score:       {baseline['F2']:.4f}      {f2_scores[-1]:.4f}      {f2_improvement:+.4f} ({f2_improvement_pct:+.1f}%)
  MCC:            {baseline['MCC']:.4f}      {mcc_scores[-1]:.4f}      {mcc_scores[-1] - baseline['MCC']:+.4f}
  S-Score:        {baseline['S_SCORE']:.4f}      {s_scores[-1]:.4f}      {s_improvement:+.4f} ({s_improvement_pct:+.1f}%)
  Accuracy:       {baseline['ACC']:.4f}      {accuracy_scores[-1]:.4f}      {accuracy_scores[-1] - baseline['ACC']:+.4f}
"""

        summary: str = f"""
{'='*70}
THESIS EVALUATION SUMMARY
{'='*70}
Experiment ID: {self.experiment_id}
Date: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

CONFIGURATION:
  Data File:       {self.full_history['config']['data_file']}
  Dataset Type:    {self.full_history['config']['dataset_type']}
  Sample Size:     {self.full_history['config']['sample_size']}
  Iterations Run:  {len(iterations)} / {self.full_history['config']['max_iterations']}
{baseline_comparison}
PERFORMANCE OVERVIEW (F1-Score):
  Initial F1:      {f1_scores[0]:.4f} (Iteration {iterations[0]['iteration']})
  Final F1:        {f1_scores[-1]:.4f} (Iteration {iterations[-1]['iteration']})
  Improvement:     {f1_scores[-1] - f1_scores[0]:+.4f}

  Best F1:         {best_f1_value:.4f} (Iteration {iterations[best_f1_index]['iteration']})
  Worst F1:        {worst_f1_value:.4f} (Iteration {iterations[worst_f1_index]['iteration']})
  Mean F1:         {sum(f1_scores) / len(f1_scores):.4f}

ADVANCED METRICS:
  F2-Score (Î²=2, Recall-Optimized):
    Initial:       {f2_scores[0]:.4f}
    Final:         {f2_scores[-1]:.4f}
    Best:          {best_f2_value:.4f} (Iteration {iterations[best_f2_index]['iteration']})
    Mean:          {sum(f2_scores) / len(f2_scores):.4f}

  Matthews Correlation Coefficient (MCC):
    Initial:       {mcc_scores[0]:.4f}
    Final:         {mcc_scores[-1]:.4f}
    Mean:          {sum(mcc_scores) / len(mcc_scores):.4f}

  S-Score (F2 + MCC, Combined Optimization Metric):
    Initial:       {s_scores[0]:.4f}
    Final:         {s_scores[-1]:.4f}
    Best:          {best_s_value:.4f} (Iteration {iterations[best_s_index]['iteration']})
    Mean:          {sum(s_scores) / len(s_scores):.4f}

  Accuracy:
    Initial:       {accuracy_scores[0]:.4f}
    Final:         {accuracy_scores[-1]:.4f}
    Mean:          {sum(accuracy_scores) / len(accuracy_scores):.4f}

PROMPT EVOLUTION:
  Initial Length:  {len(iterations[0]['prompt'])} chars
  Final Length:    {len(iterations[-1]['prompt'])} chars
  Growth:          {len(iterations[-1]['prompt']) - len(iterations[0]['prompt']):+d} chars

DATA FILES FOR THESIS:
  Quantitative Analysis:  {self.csv_path}
  Qualitative Analysis:   {self.json_path}
  Summary Report:         {self.summary_path}

INITIAL PROMPT:
{iterations[0]['prompt'][:300]}...

FINAL PROMPT:
{iterations[-1]['prompt'][:300]}...

{'='*70}
"""

        with open(self.summary_path, 'w', encoding='utf-8') as summary_file:
            summary_file.write(summary)

        logging.info("Summary generation complete")
        logging.info(f"Summary saved to: {self.summary_path}")
        print(summary)


def main() -> None:
    """
    Execute the complete three-phase iterative prompt optimization experiment.

    This function implements the experimental pipeline as shown in the
    methodological flowchart with STRICT DATA SEPARATION:

    Phase 1: Initialization
        - Initialize data logging infrastructure
        - Load and prepare dataset (gutefrage or HOCON)
        - CRITICAL: Stratified train/val/test split (prevents data leakage)
        - Run baseline evaluation with initial prompt on VALIDATION set (iteration 0)

    Phase 2: Iterative Optimization Cycle
        - Execute optimization loop (up to MAX_ITERATIONS) on VALIDATION set
        - Gemma inference with batch processing
        - Calculate metrics (F1, F2, MCC, Accuracy)
        - Check convergence (max iterations or perfect score)
        - Perform error analysis and example extraction
        - Use optimizer LLM for strategic few-shot prompt generation
        - Track best-performing prompt based on F2-Score
        - Track iteration times and calculate ETA
        - TEST SET IS NEVER TOUCHED during this phase

    Phase 3: Final Validation
        - Single evaluation on held-out TEST set with best prompt
        - Generate summary statistics and visualizations
        - Report final test set performance (genuine generalization)

    CRITICAL SCIENTIFIC REQUIREMENT:
        The test set is held out completely during optimization (Phase 2) and
        evaluated exactly once at the end (Phase 3). This ensures reported
        performance represents genuine generalization, not overfitting.
    """
    logging.info("=" * 70)
    logging.info("ITERATIVE PROMPT OPTIMIZATION FRAMEWORK")
    logging.info("Three-Phase Architecture: Baseline â†’ Optimization â†’ Final Test")
    logging.info("=" * 70)

    logger: ResultsLogger = ResultsLogger()

    # Phase 1: Initialization - Load and split dataset
    logging.info(f"Loading {DATASET_TYPE} dataset from: {DATA_FILE}")
    try:
        df = load_and_prepare_data(DATA_FILE, dataset_type=DATASET_TYPE, sample_size=SAMPLE_SIZE)
    except FileNotFoundError:
        logging.error(f"Data file not found: {DATA_FILE}")
        return
    except Exception as e:
        logging.error(f"Error loading data: {e}")
        return

    # CRITICAL: Stratified split to prevent data leakage
    val_df, test_df = split_data(df, val_size=0.5, test_size=0.5, random_state=42)

    # Phase 1: Baseline Evaluation on VALIDATION set (Iteration 0)
    current_prompt: str = INITIAL_PROMPT
    baseline_metrics = run_baseline_evaluation(val_df, current_prompt)

    # Save baseline to logger
    logger.save_baseline(baseline_metrics)

    # Log baseline as iteration 0
    errors_baseline: List[Dict[str, Any]] = []
    for idx, (yt, yp) in enumerate(zip(baseline_metrics['y_true'], baseline_metrics['y_pred'])):
        if yt != yp:
            error_type = "FP" if yp == 1 else "FN"
            errors_baseline.append({
                "type": error_type,
                "text": val_df.iloc[idx]['text'],
                "true_label": yt,
                "pred_label": yp
            })

    logger.log_iteration(0, baseline_metrics, current_prompt, errors_baseline)
    logging.info("Baseline (Iteration 0) saved to disk")

    # Phase 2: Iterative Optimization Cycle on VALIDATION set
    logging.info("=" * 70)
    logging.info("PHASE 2: ITERATIVE OPTIMIZATION CYCLE (on validation set)")
    logging.info("âš ï¸  TEST SET IS HELD OUT - will be evaluated ONCE at the end")
    logging.info("=" * 70)

    iteration_times: List[float] = []

    # Track best prompt based on S-Score (F2 + MCC combined metric)
    best_s_score: float = baseline_metrics['S_SCORE']
    best_prompt: str = current_prompt
    best_iteration: int = 0

    for iteration in range(1, MAX_ITERATIONS + 1):
        iteration_start_time: float = time.time()

        logging.info("=" * 70)
        logging.info(f"ITERATION {iteration}/{MAX_ITERATIONS}")
        logging.info("=" * 70)
        logging.info(f"Prompt preview: {current_prompt[:100]}...")

        y_true: List[int] = []
        y_pred: List[int] = []
        errors: List[Dict[str, Any]] = []

        logging.info("Running Gemma inference on validation set...")
        for _, row in tqdm(val_df.iterrows(), total=len(val_df), desc="Classification progress"):
            text: str = row['text']
            true_label: int = row['label']

            result: Dict[str, Any] = classify_text(text, current_prompt)
            pred_label: int = result['label']

            y_true.append(true_label)
            y_pred.append(pred_label)

            if pred_label != true_label:
                error_type: str = "FP" if pred_label == 1 else "FN"
                errors.append({
                    "type": error_type,
                    "text": text,
                    "true_label": true_label,
                    "pred_label": pred_label
                })

        # Calculate comprehensive metrics using centralized function
        iteration_metrics = calculate_metrics(y_true, y_pred)
        iteration_metrics['y_true'] = y_true
        iteration_metrics['y_pred'] = y_pred

        logging.info("Iteration results (validation set):")
        logging.info(f"  F1-Score:  {iteration_metrics['F1']:.4f}")
        logging.info(f"  F2-Score:  {iteration_metrics['F2']:.4f}")
        logging.info(f"  MCC:       {iteration_metrics['MCC']:.4f}")
        logging.info(f"  S-Score:   {iteration_metrics['S_SCORE']:.4f} (F2 + MCC)")
        logging.info(f"  Accuracy:  {iteration_metrics['ACC']:.4f}")
        logging.info(f"  Errors:    {len(errors)}/{len(val_df)}")

        # Track best prompt based on S-Score (combined optimization metric)
        if iteration_metrics['S_SCORE'] > best_s_score:
            best_s_score = iteration_metrics['S_SCORE']
            best_prompt = current_prompt
            best_iteration = iteration
            logging.info(f"  ðŸŽ¯ NEW BEST S-Score: {best_s_score:.4f} (Iteration {iteration})")

        logger.log_iteration(iteration, iteration_metrics, current_prompt, errors)
        logging.info(f"Iteration {iteration} data saved to disk")

        # ETA calculation
        iteration_elapsed_time: float = time.time() - iteration_start_time
        iteration_times.append(iteration_elapsed_time)

        if iteration < MAX_ITERATIONS:
            avg_time_per_iteration: float = sum(iteration_times) / len(iteration_times)
            remaining_iterations: int = MAX_ITERATIONS - iteration
            estimated_time_remaining: float = avg_time_per_iteration * remaining_iterations

            logging.info(f"Iteration completed in {iteration_elapsed_time:.1f}s")
            logging.info(f"Average time per iteration: {avg_time_per_iteration:.1f}s")
            logging.info(f"Estimated time remaining: {format_time_remaining(estimated_time_remaining)}")

        # Convergence check
        if iteration == MAX_ITERATIONS:
            logging.info("Maximum iterations reached. Optimization complete.")
            break

        # Error analysis and prompt optimization
        if errors:
            logging.info(f"Analyzing {min(len(errors), MAX_ERRORS_TO_ANALYZE)} errors for prompt refinement...")
            error_text: str = "\n".join([
                f"[{e['type']}] Text: '{e['text']}' | True: {e['true_label']} | Pred: {e['pred_label']}"
                for e in errors[:MAX_ERRORS_TO_ANALYZE]
            ])

            try:
                logging.info("Invoking Optimizer LLM for strategic few-shot selection...")
                current_prompt = generate_improved_prompt(current_prompt, error_text)
                logging.info("Improved prompt generated successfully")
            except Exception as e:
                logging.warning(f"Error during prompt generation: {e}")
                logging.warning("Continuing with current prompt")
        else:
            logging.info("Perfect classification achieved. Stopping optimization early.")
            break

    # Phase 3: Final Test Set Evaluation (CRITICAL - held-out data)
    logging.info("=" * 70)
    logging.info("PHASE 3: FINAL TEST SET EVALUATION")
    logging.info("âš ï¸  CRITICAL: This is the ONLY evaluation on the held-out test set")
    logging.info("=" * 70)
    logging.info(f"Best prompt found at Iteration {best_iteration} (S-Score={best_s_score:.4f} on validation)")
    logging.info(f"Now evaluating on test set ({len(test_df)} samples)...")

    # Step 1: Evaluate OPTIMIZED prompt on test set
    logging.info("\n[1/2] Evaluating OPTIMIZED prompt on test set...")
    y_true_test: List[int] = []
    y_pred_optimized: List[int] = []

    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc="Optimized prompt test"):
        text: str = row['text']
        true_label: int = row['label']

        result: Dict[str, Any] = classify_text(text, best_prompt)
        pred_label: int = result['label']

        y_true_test.append(true_label)
        y_pred_optimized.append(pred_label)

    # Calculate optimized test metrics
    optimized_test_metrics = calculate_metrics(y_true_test, y_pred_optimized)

    # Step 2: Evaluate BASELINE prompt on test set (CRITICAL for fair comparison)
    logging.info("\n[2/2] Evaluating BASELINE prompt on test set (for fair comparison)...")
    y_pred_baseline: List[int] = []

    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc="Baseline prompt test"):
        text: str = row['text']

        result: Dict[str, Any] = classify_text(text, INITIAL_PROMPT)
        pred_label: int = result['label']

        y_pred_baseline.append(pred_label)

    # Calculate baseline test metrics
    baseline_test_metrics = calculate_metrics(y_true_test, y_pred_baseline)

    # Step 3: Statistical Significance Testing
    logging.info("\n" + "=" * 70)
    logging.info("STATISTICAL SIGNIFICANCE TESTING")
    logging.info("=" * 70)

    # McNemar's test for statistical significance
    logging.info("\nRunning McNemar's test (paired predictions)...")
    mcnemar_p, mcnemar_interp = mcnemar_test(y_true_test, y_pred_baseline, y_pred_optimized)
    logging.info(f"McNemar's test p-value: {mcnemar_p:.6f}")
    logging.info(f"Interpretation: {mcnemar_interp}")

    # Bootstrap confidence intervals for key metrics
    logging.info("\nCalculating 95% confidence intervals (bootstrap, n=1000)...")

    # Baseline confidence intervals
    baseline_f2_est, baseline_f2_lower, baseline_f2_upper = bootstrap_confidence_interval(
        y_true_test, y_pred_baseline, 'F2', n_bootstrap=1000
    )
    baseline_mcc_est, baseline_mcc_lower, baseline_mcc_upper = bootstrap_confidence_interval(
        y_true_test, y_pred_baseline, 'MCC', n_bootstrap=1000
    )
    baseline_s_est, baseline_s_lower, baseline_s_upper = bootstrap_confidence_interval(
        y_true_test, y_pred_baseline, 'S_SCORE', n_bootstrap=1000
    )

    # Optimized confidence intervals
    optimized_f2_est, optimized_f2_lower, optimized_f2_upper = bootstrap_confidence_interval(
        y_true_test, y_pred_optimized, 'F2', n_bootstrap=1000
    )
    optimized_mcc_est, optimized_mcc_lower, optimized_mcc_upper = bootstrap_confidence_interval(
        y_true_test, y_pred_optimized, 'MCC', n_bootstrap=1000
    )
    optimized_s_est, optimized_s_lower, optimized_s_upper = bootstrap_confidence_interval(
        y_true_test, y_pred_optimized, 'S_SCORE', n_bootstrap=1000
    )

    # Step 4: Report Results
    logging.info("\n" + "=" * 70)
    logging.info("FINAL TEST SET RESULTS (Genuine Generalization Performance)")
    logging.info("=" * 70)

    logging.info("\nBASELINE (Initial Prompt) Performance on Test Set:")
    logging.info(f"  F1-Score:  {baseline_test_metrics['F1']:.4f}")
    logging.info(f"  F2-Score:  {baseline_test_metrics['F2']:.4f} [{baseline_f2_lower:.4f}, {baseline_f2_upper:.4f}]")
    logging.info(f"  MCC:       {baseline_test_metrics['MCC']:.4f} [{baseline_mcc_lower:.4f}, {baseline_mcc_upper:.4f}]")
    logging.info(f"  S-Score:   {baseline_test_metrics['S_SCORE']:.4f} [{baseline_s_lower:.4f}, {baseline_s_upper:.4f}]")
    logging.info(f"  Accuracy:  {baseline_test_metrics['ACC']:.4f}")

    logging.info("\nOPTIMIZED (Best Prompt) Performance on Test Set:")
    logging.info(f"  F1-Score:  {optimized_test_metrics['F1']:.4f}")
    logging.info(f"  F2-Score:  {optimized_test_metrics['F2']:.4f} [{optimized_f2_lower:.4f}, {optimized_f2_upper:.4f}]")
    logging.info(f"  MCC:       {optimized_test_metrics['MCC']:.4f} [{optimized_mcc_lower:.4f}, {optimized_mcc_upper:.4f}]")
    logging.info(f"  S-Score:   {optimized_test_metrics['S_SCORE']:.4f} [{optimized_s_lower:.4f}, {optimized_s_upper:.4f}]")
    logging.info(f"  Accuracy:  {optimized_test_metrics['ACC']:.4f}")

    # Calculate improvements
    f2_improvement = optimized_test_metrics['F2'] - baseline_test_metrics['F2']
    f2_improvement_pct = (f2_improvement / baseline_test_metrics['F2'] * 100) if baseline_test_metrics['F2'] != 0 else 0
    mcc_improvement = optimized_test_metrics['MCC'] - baseline_test_metrics['MCC']
    s_score_improvement = optimized_test_metrics['S_SCORE'] - baseline_test_metrics['S_SCORE']
    s_score_improvement_pct = (s_score_improvement / baseline_test_metrics['S_SCORE'] * 100) if baseline_test_metrics['S_SCORE'] != 0 else 0

    logging.info("\nIMPROVEMENT (Optimized vs Baseline):")
    logging.info(f"  F2-Score:  {f2_improvement:+.4f} ({f2_improvement_pct:+.1f}%)")
    logging.info(f"  MCC:       {mcc_improvement:+.4f}")
    logging.info(f"  S-Score:   {s_score_improvement:+.4f} ({s_score_improvement_pct:+.1f}%)")
    logging.info(f"  Statistical Significance: p = {mcnemar_p:.6f} ({mcnemar_interp})")
    logging.info("=" * 70)

    # Save comprehensive test results to a separate file for thesis reporting
    test_results_path = logger.results_dir / f"{logger.experiment_id}_TEST_RESULTS.json"
    test_results = {
        "experiment_id": logger.experiment_id,
        "test_set_size": int(len(test_df)),
        "best_iteration": int(best_iteration),
        "best_val_s_score": float(best_s_score),

        "baseline_test_metrics": {
            "F1": float(baseline_test_metrics['F1']),
            "F2": float(baseline_test_metrics['F2']),
            "F2_CI_95": [float(baseline_f2_lower), float(baseline_f2_upper)],
            "MCC": float(baseline_test_metrics['MCC']),
            "MCC_CI_95": [float(baseline_mcc_lower), float(baseline_mcc_upper)],
            "S_SCORE": float(baseline_test_metrics['S_SCORE']),
            "S_SCORE_CI_95": [float(baseline_s_lower), float(baseline_s_upper)],
            "ACC": float(baseline_test_metrics['ACC'])
        },

        "optimized_test_metrics": {
            "F1": float(optimized_test_metrics['F1']),
            "F2": float(optimized_test_metrics['F2']),
            "F2_CI_95": [float(optimized_f2_lower), float(optimized_f2_upper)],
            "MCC": float(optimized_test_metrics['MCC']),
            "MCC_CI_95": [float(optimized_mcc_lower), float(optimized_mcc_upper)],
            "S_SCORE": float(optimized_test_metrics['S_SCORE']),
            "S_SCORE_CI_95": [float(optimized_s_lower), float(optimized_s_upper)],
            "ACC": float(optimized_test_metrics['ACC'])
        },

        "improvement": {
            "F2_absolute": float(f2_improvement),
            "F2_relative_percent": float(f2_improvement_pct),
            "MCC_absolute": float(mcc_improvement),
            "S_SCORE_absolute": float(s_score_improvement),
            "S_SCORE_relative_percent": float(s_score_improvement_pct)
        },

        "statistical_significance": {
            "mcnemar_p_value": float(mcnemar_p),
            "mcnemar_interpretation": mcnemar_interp,
            "is_significant_at_05": bool(mcnemar_p < 0.05)
        },

        "prompts": {
            "baseline_prompt": INITIAL_PROMPT,
            "optimized_prompt": best_prompt
        },

        "note": "FAIR COMPARISON: Both baseline and optimized prompts evaluated on the SAME held-out test set. Statistical significance confirmed via McNemar's test. Confidence intervals calculated via bootstrap (n=1000)."
    }

    with open(test_results_path, 'w', encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False)

    logging.info(f"\nTest results saved to: {test_results_path}")

    # Phase 3: Final Analysis and Reporting
    logging.info("=" * 70)
    logging.info("GENERATING SUMMARY AND VISUALIZATIONS")
    logging.info("=" * 70)
    logger.generate_summary()

    logging.info("=" * 70)
    logging.info("GENERATING VISUALIZATIONS")
    logging.info("=" * 70)
    try:
        plot_results(logger.csv_path)
    except Exception as e:
        logging.error(f"Error generating visualization: {e}")
        logging.error("Continuing without visualization")

    logging.info("=" * 70)
    logging.info("EXPERIMENT COMPLETE")
    logging.info("=" * 70)
    logging.info(f"âœ… Validation performance tracked across {MAX_ITERATIONS} iterations")
    logging.info(f"âœ… Test set evaluated with BOTH baseline and optimized prompts")
    logging.info(f"âœ… Baseline test S-Score:   {baseline_test_metrics['S_SCORE']:.4f} [{baseline_s_lower:.4f}, {baseline_s_upper:.4f}]")
    logging.info(f"âœ… Optimized test S-Score:  {optimized_test_metrics['S_SCORE']:.4f} [{optimized_s_lower:.4f}, {optimized_s_upper:.4f}]")
    logging.info(f"âœ… Improvement:             {s_score_improvement:+.4f} ({s_score_improvement_pct:+.1f}%) - {mcnemar_interp}")
    logging.info("=" * 70)
    logging.info("THESIS REPORTING:")
    logging.info(f"  - Validation results: {logger.csv_path}")
    logging.info(f"  - Test results (with statistical tests): {test_results_path}")
    logging.info(f"  - Full provenance: {logger.json_path}")
    logging.info("\nSCIENTIFIC VALIDITY CHECKLIST:")
    logging.info(f"  âœ… Data split: Stratified train/val/test (no leakage)")
    logging.info(f"  âœ… Fair comparison: Both prompts evaluated on same test set")
    logging.info(f"  âœ… Statistical rigor: McNemar's test + bootstrap CIs")
    logging.info(f"  âœ… Reproducibility: random_state=42 for data splits")


if __name__ == "__main__":
    main()
